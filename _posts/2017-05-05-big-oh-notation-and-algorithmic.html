---
title: Big Oh notation and Algorithmic Complexity analysis
layout: post
date: '2017-05-05T15:18:00.002-07:00'
author: Shiva Gaire
tags:
- Big Oh notations
- Algorithm
- Theoritical computer science
modified_time: '2017-05-06T00:45:15.407-07:00'
thumbnail: https://1.bp.blogspot.com/-q_If4WoaJTA/WQz5mDhm2QI/AAAAAAAATGg/AWr6IyaHI98WN5aZL0XY5hH4Rmn2XmghACLcB/s72-c/Screenshot%2Bfrom%2B2017-05-06%2B04-00-15.png
blogger_id: tag:blogger.com,1999:blog-2622751468229364031.post-2253847378467081415
blogger_orig_url: http://geeksambhu.blogspot.com/2017/05/big-oh-notation-and-algorithmic.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><h3 style="text-align: left;">Introduction</h3><div>When executing a particular task using algorithms, one common thing that hits every problem solver brain is the efficient and fast algorithm to solve that problem. But, what do exactly fast and efficient means? Is it the measure of real processing time? No, it's not the measure of real time like second and minutes because the old computer with Pentium processor may take a long time to process the same algorithm than the new Intel i3 processor, or, A&nbsp;bad algorithm written in Assembly may execute faster than a good algorithm written in python. So, run time of program can't be considered to measure algorithmic efficiency.&nbsp;</div><div><br /></div><div>Hence, to measure the algorithmic efficiency, the concept of <b>Asymptotic Complexity </b>of a program and a notation for describing this called <b>Big Oh </b>&nbsp;was introduced. <b>Big Oh </b>is the worst case, <b>Big Omega </b>and<b> Big Theta </b>are best and average case notations. Worst case means we are mostly unlucky for that problem domain , i.e precondition of task do not favour us. Complexity analysis is a tool that allows us to explain how an algorithm behaves as the input grows larger.</div><div><br /></div><h3 style="text-align: left;">Scenarios</h3><div>Sorting an array of size 10000 compared to array of size 100 and analyzing how run time of the program grows.</div><div>Now, Lets dive deeper,</div><div><b>Counting the number of character in a string:</b></div><div><br /></div><div>&nbsp; &nbsp; One simplest approach is traversing through whole string letter by letter and incrementing a counter variable by 1. This algorithmic approach run in linear time with respect to number of character '<b>n</b>' in the string, i.e, it runs in <b>O(n).&nbsp;</b></div><div><b>Why ??</b></div><div>Using this approach the time required to traverse the entire string is proportional to number of character in string, i.e time required to traverse string with 40 character is twice the time required to trverse string with 20 character as the amount of time to look individual character is same.</div><div><br /></div><div><br /></div><div>Another approach is, declaring a variable and storing the number of character in a variable say "length" early in the program, i.e, before storing the very first character. Now, there is no need to look at string, instead one have to check the value of that variable. The accessing of such variable is generally asymptotically constant time operation, or&nbsp;<b>O(1). </b>This is because Asymptotic means "How the run time change as input grows". In this approach the length of string whether it has one character or thousandsof character, the only thing we need to do is to find string length and which can be done by reading the "length" variable and the reading time for this variable is constant regardless of string size. Hence this approach can be referred as running in constant time.</div><div>The <b>O(1) </b>does not change with the size of inputs.</div><div><br /></div><h3 style="text-align: left;">Variations</h3><div>There are many different Big Oh runtimes like <b>O(n), O(n</b>²) etc</div><div><b>O(n</b>²) are asymptotically slower than&nbsp;<b>O(n) i.e if&nbsp;</b>n grows &nbsp;<b>O(n</b>²) will take more time than&nbsp;<b>O(n).&nbsp;</b></div><div>This dosen't means <b>O(n) </b>always run faster, maybe if input is smaller then &nbsp;<b>O(n</b>²) may work faster yet unnoticed</div><div>Similarly, we may have logarithmic&nbsp;<b>O(log(n))</b> for some cases like in Binary search. Binary search cut the array size in half with each operation.</div><div>Simpler Program can be anlysed by counting number of nested loop.</div><div style="text-align: left;"><ul style="text-align: left;"><li>A single loop over n items has&nbsp;<b>O(n)</b> complexity.</li><li>A loop within a loop has&nbsp;<b>O(n</b>²) complexity</li></ul><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-q_If4WoaJTA/WQz5mDhm2QI/AAAAAAAATGg/AWr6IyaHI98WN5aZL0XY5hH4Rmn2XmghACLcB/s1600/Screenshot%2Bfrom%2B2017-05-06%2B04-00-15.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img alt="Different variations of Big Oh" border="0" height="272" src="https://1.bp.blogspot.com/-q_If4WoaJTA/WQz5mDhm2QI/AAAAAAAATGg/AWr6IyaHI98WN5aZL0XY5hH4Rmn2XmghACLcB/s320/Screenshot%2Bfrom%2B2017-05-06%2B04-00-15.png" title="Big Oh notation" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Big Oh Variant</td></tr></tbody></table><div><br /></div></div></div>